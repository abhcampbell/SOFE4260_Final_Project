{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\abain\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\abain\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import pandas as pd\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer \n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","import sklearn\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import LabelBinarizer\n","\n","import os\n","\n","from keras import regularizers\n","from keras import layers\n","from keras.preprocessing.text import Tokenizer\n","from keras.models import Sequential\n","from keras.models import load_model\n","from keras.callbacks import EarlyStopping\n","from keras_preprocessing.sequence import pad_sequences\n","\n","import numpy as np"]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>is upset that he can't update his Facebook by ...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>@Kenichan I dived many times for the ball. Man...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>my whole body feels itchy and like its on fire</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>@nationwideclass no, it's not behaving at all....</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   target                                               text\n","0       0  @switchfoot http://twitpic.com/2y1zl - Awww, t...\n","1       0  is upset that he can't update his Facebook by ...\n","2       0  @Kenichan I dived many times for the ball. Man...\n","3       0    my whole body feels itchy and like its on fire \n","4       0  @nationwideclass no, it's not behaving at all...."]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["# First we can import the data and setup the dataframe with the correct column labels\n","# Target column represents sentiment. 0: Negative, 4: Positive\n","# This is needed if .csv has not been created yet, otherwise ignore\n","\n","columnLabels = [\"target\", \"ids\", \"date\", \"flag\", \"user\", \"text\"]\n","dataEncoding = \"ISO-8859-1\"\n","rawData = pd.read_csv('twitterKaggleData.csv', encoding=dataEncoding, names=columnLabels)\n","\n","rawData.head(5)\n","\n","# Here we can do some inital data processing and remove the columns that are not relevant to the model we are trying to make\n","\n","data = rawData.drop(columns=[\"ids\", \"date\", \"flag\", \"user\"])\n","data.head(5)"]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>You shoulda got David Carr Third Day</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>upset update Facebook texting might cry result...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>I dived many time Managed save The rest go bound</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>whole body feel itchy like fire</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>behaving I see</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   target                                               text\n","0       0               You shoulda got David Carr Third Day\n","1       0  upset update Facebook texting might cry result...\n","2       0   I dived many time Managed save The rest go bound\n","3       0                    whole body feel itchy like fire\n","4       0                                     behaving I see"]},"execution_count":3,"metadata":{},"output_type":"execute_result"}],"source":["# As we can see in the text, there are links, usernames, and special characters that we do not want when we are training so we can remove them\n","# We can then perform the best practice preprocessing which is tokenization, lower casing, stop word removal, and lemmatize\n","# We chose lemmatization over stemming as stemming can sometimes be inaccurate\n","\n","def processText(text):\n","    lemmatizer = WordNetLemmatizer()\n","    arr = []\n","\n","    for token in text.split():\n","        if token not in stopwords.words(\"english\") and token.isalnum():\n","            word = lemmatizer.lemmatize(token)\n","            arr.append(word)\n","\n","    return \" \".join(arr)\n","\n","# If csv file exists, use it, otherwise process text\n","if os.path.exists('./processedTwitterData.csv'):\n","    data = pd.read_csv('processedTwitterData.csv')\n","\n","    # Ensures that all values are of string as pd.read_csv automatically will convert the text into a type such as float or int\n","    data.text = data.text.astype(str)\n","else:\n","    length = len(data.text)\n","    curr = 0\n","\n","    for index, text in enumerate(data.text):\n","        newText = processText(text)\n","        curr += 1\n","        data.at[index, 'text'] = newText\n","\n","        print(\"Processing Progress: \" + str(curr) + \"/\" + str(length), end=\"\\r\")\n","\n","    # Save the process after it's complete so that the .csv can be instead since the kernel does not save session to session and the processing takes 2-3 hours\n","    data.to_csv('processedTwitterData.csv', index=False)\n","\n","data.head(5)"]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>target</th>\n","      <th>text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>You shoulda got David Carr Third Day</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>0</td>\n","      <td>upset update Facebook texting might cry result...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0</td>\n","      <td>I dived many time Managed save The rest go bound</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","      <td>whole body feel itchy like fire</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>behaving I see</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>1599995</th>\n","      <td>1</td>\n","      <td>Just woke Having school best feeling ever</td>\n","    </tr>\n","    <tr>\n","      <th>1599996</th>\n","      <td>1</td>\n","      <td>Very cool hear old Walt</td>\n","    </tr>\n","    <tr>\n","      <th>1599997</th>\n","      <td>1</td>\n","      <td>Are ready MoJo Ask detail</td>\n","    </tr>\n","    <tr>\n","      <th>1599998</th>\n","      <td>1</td>\n","      <td>Happy 38th Birthday boo alll Tupac Amaru Shakur</td>\n","    </tr>\n","    <tr>\n","      <th>1599999</th>\n","      <td>1</td>\n","      <td>happy</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>1600000 rows Ã— 2 columns</p>\n","</div>"],"text/plain":["         target                                               text\n","0             0               You shoulda got David Carr Third Day\n","1             0  upset update Facebook texting might cry result...\n","2             0   I dived many time Managed save The rest go bound\n","3             0                    whole body feel itchy like fire\n","4             0                                     behaving I see\n","...         ...                                                ...\n","1599995       1          Just woke Having school best feeling ever\n","1599996       1                            Very cool hear old Walt\n","1599997       1                          Are ready MoJo Ask detail\n","1599998       1    Happy 38th Birthday boo alll Tupac Amaru Shakur\n","1599999       1                                              happy\n","\n","[1600000 rows x 2 columns]"]},"execution_count":4,"metadata":{},"output_type":"execute_result"}],"source":["# Since the original Kaggle Twitter Data only has labels 0 for negative and 4 for positive, we can convert it to a binary 0 for negative 1 for positive so we can use a binary classification loss function\n","\n","data['target'].replace({4: 1}, inplace=True)\n","\n","data"]},{"cell_type":"code","execution_count":5,"metadata":{},"outputs":[],"source":["# Now that we have our pre-processed data we can undergo another important step in NLP: word embeddings\n","# Here we transform our words into vector representations as it allows for our model to be able to more easily figure out relations that words have to each other and the context in which the word is being used\n","\n","LENGTH = 280\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(data.text)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Run if you want to use the model that we trained\n","\n","seqModel = load_model(\"trainedSequentialModel\")\n","seqModel.summary() "]},{"cell_type":"code","execution_count":6,"metadata":{},"outputs":[],"source":["# We can split our data up into training data and testing date, from there we can turn our training and testing text into vector representations based on the Tokenizer we fit earlier\n","\n","trainingData, testingData, trainingLabels, testingLabels = train_test_split(data.text, data.target, test_size=0.2, train_size=0.1)\n","\n","# We can pad the sequences to make them the same size so the model can train on it\n","trainingData = pad_sequences(tokenizer.texts_to_sequences(trainingData), maxlen=LENGTH) # Convert the data to vector form\n","testingData = pad_sequences(tokenizer.texts_to_sequences(testingData), maxlen=LENGTH) # Convert the data to vector form"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, None, 280)         58699760  \n","                                                                 \n"," lstm_1 (LSTM)               (None, 200)               384800    \n","                                                                 \n"," flatten_1 (Flatten)         (None, 200)               0         \n","                                                                 \n"," dense_1 (Dense)             (None, 200)               40200     \n","                                                                 \n"," dense_2 (Dense)             (None, 1)                 201       \n","                                                                 \n","=================================================================\n","Total params: 59,124,961\n","Trainable params: 59,124,961\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["# Run if you are preparing to train a new model\n","\n","BATCH_SIZE = 2048\n","\n","seqModel = Sequential()\n","# We can add in an embedding layer that will extract the similar representations of words from their vector form\n","seqModel.add(layers.Embedding(len(tokenizer.word_index) + 1, 280)) \n","# Size of LTSM layer has been selected through experimentation\n","seqModel.add(layers.LSTM(200, dropout=0.2))\n","seqModel.add(layers.Flatten())\n","# Determined ReLU is best from trial and error\n","# Determine one Dense layer is best from trial and error\n","seqModel.add(layers.Dense(1, activation='relu'))\n","\n","seqModel.compile(optimizer='Adam', loss='mse', metrics=['accuracy'])\n","\n","seqModel.summary()"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/4\n","625/625 [==============================] - 5654s 9s/step - loss: 0.1736 - accuracy: 0.7433\n","Epoch 2/4\n","625/625 [==============================] - 5554s 9s/step - loss: 0.1518 - accuracy: 0.7796\n","Epoch 3/4\n","625/625 [==============================] - 5324s 9s/step - loss: 0.1405 - accuracy: 0.7973\n","Epoch 4/4\n","625/625 [==============================] - 5426s 9s/step - loss: 0.1311 - accuracy: 0.8121\n"]}],"source":["# DO NOT RUN THIS UNLESS YOU YOU WANT TO TRAIN THE MODEL, TAKES A VERY LONG TIME\n","callback = EarlyStopping(monitor='loss', patience=1)\n","\n","history = seqModel.fit(trainingData, trainingLabels, epochs=4, batch_size=BATCH_SIZE, callbacks=callback)\n","\n","# Various attempt results and what we are doing next:\n"," \n","# Sigmoid Runs\n","# 0.05 data split, 4 epochs, 2048 batch size: loss: 0.3991 - accuracy: 0.8196 -> try to increase epochs and training size\n","# 0.1 data split, 12 epohcs, 2048 batch size: loss: 0.2537 - accuracy: 0.8794 -> try to increase epochs \n","# 0.1 data split, 20 epohcs, 2048 batch size: loss: 0.1451 - accuracy: 0.9329 -> good accuracy on testing data but only about 70% accuracy when testing so overfitting. Lower epochs and increasing training data\n","# 0.2 data split, 10 epochs, 2048 batch size: loss: 0.2831 - accuracy: 0.8662 -> accuracy on testing data is about 73% so generalizing better than before. Increase epochs slightly and increase training data size\n","# 0.3 data split, 12 epochs, 2048 batch size: loss: 0.2669 - accuracy: 0.8735 338 minutes -> still only 73% accurate despite increased epochs and data size. Lower epochs and data again and also batch size\n","# 0.2 data split, 10 epochs 512 batch size: loss: 0.2215 - accuracy: 0.8967 268 minutes -> still only 73% accurate, so its probably overfitting. Increasing data and lower epochs\n","# 0.4 data split, 8 epochs, 512 batch size: 0.2591 - accuracy: 0.8789 327 minutes -> 74% accurate, obvious at this point its not data. Go back down to 0.1 data, 4 epochs, and try to increase LTSM layer from 100 to 200\n","# 0.1 data split, 4 epochs 512 batch size: loss: 0.3542 - accuracy: 0.8363 -> 74% accurate with way less data and much quicker training time. Increase data amount and see if any improvements\n","# 0.2 data split, 4 epochs, 512 batch size (encountered errors so raised size and seemed to fix): 0.3647 - accuracy: 0.8307 -> 177 mintes, increase data and see what happens\n","# 0.4 data split, 4 epochs, 2048 batch size: loss: 0.3974 - accuracy: 0.8139 -> ~75% accuracy when evaluting\n","# seeing if adding another dense layer or two will improve the accuracy with a 0.1 data split to see if there are improvements. did this last minute because of neural network slides\n","# 0.1 data split, 4 epochs 512 batch size: 2048 loss:0.4028 - accuracy: 0.8167, so the extra dense layer is worse when comparing it to the previous attempt with 10% of the data\n","\n","# After doing Sigmoid for a while and not seeing any improvements we did ReLU\n","\n","# ReLU Runs: \n","# 0.1 data split, 4 epochs 512 batch size: 2048: loss: 0.1303 - accuracy: 0.8175, accuracy was about 73% but trained waaaay faster so maybe we can retry with more data and see\n","# 0.3 data split, 4 epochs 512 batch size: 2048: loss: 0.1328 - accuracy: 0.8115, accuracy is 75% but again trained much faster\n","# 0.8 data split, 4 epochs, 512 batch size: 2048: loss: 0.1311 - accuracy: 0.8121, accuracy was 76% which is a new record! this can be the final model."]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["157/157 [==============================] - 456s 3s/step - loss: 0.1641 - accuracy: 0.7633\n","test loss, test acc: [0.16414092481136322, 0.7632781267166138]\n"]}],"source":["# Evaluate the model here\n","\n","results = seqModel.evaluate(testingData, testingLabels, batch_size=BATCH_SIZE)\n","print(\"test loss, test acc:\", results)"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["WARNING:absl:Found untraced functions such as _update_step_xla, lstm_cell_layer_call_fn, lstm_cell_layer_call_and_return_conditional_losses while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"]},{"name":"stdout","output_type":"stream","text":["INFO:tensorflow:Assets written to: trainedSequentialModel\\assets\n"]},{"name":"stderr","output_type":"stream","text":["INFO:tensorflow:Assets written to: trainedSequentialModel\\assets\n"]}],"source":["# Since the model takes a really long time to train we can save it here after its done\n","\n","seqModel.save('trainedSequentialModel')"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPKEUUPx1377maqnIz0FxFt","mount_file_id":"1EDw7oTnmp_OHzesIz0JFBnr5Ag7FzjUo","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.3"}},"nbformat":4,"nbformat_minor":0}
