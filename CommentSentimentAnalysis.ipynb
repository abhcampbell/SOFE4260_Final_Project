{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Follow the instructions contained in the comments of each code block to get the sentiment for your selceted video"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to\n","[nltk_data]     C:\\Users\\abain\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package wordnet to\n","[nltk_data]     C:\\Users\\abain\\AppData\\Roaming\\nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n"]}],"source":["import pandas as pd\n","\n","import os\n","\n","from googleapiclient.discovery import build\n","\n","import nltk\n","from nltk.corpus import stopwords\n","from nltk.stem import WordNetLemmatizer \n","nltk.download('stopwords')\n","nltk.download('wordnet')\n","\n","from keras.preprocessing.text import Tokenizer\n","from keras_preprocessing.sequence import pad_sequences\n","from keras.models import load_model\n","\n","import numpy as np"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["# First input your key and the videos URL\n","\n","key = 'AIzaSyBPVjWSZ7gY_1eT70KFeSIn4Yc6zULaLEI' # Insert Youtube DATA API key here, get one here: https://console.cloud.google.com/marketplace/product/google/youtube.googleapis.com?q=search&referrer=search&project=streetview-api-key\n","vidURL = 'https://www.youtube.com/watch?v=7Vj5M0qKh8g' # Insert video URL here\n","MAX_RESULTS = 10000 # Cap the amount of comments it will retrieve"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[],"source":["# Here we collect the comments from the Youtube video, just input a URL as a string into the id string value\n","\n","youtube = build('youtube', 'v3', developerKey=key)\n","id = vidURL.split('=')[1]\n","\n","res = youtube.commentThreads().list(\n","  part = 'snippet',\n","  videoId = id,\n","  maxResults = MAX_RESULTS\n",").execute()\n","\n","comments = []\n","curr = 0\n","\n","for item in res['items']:\n","    comment = item['snippet']['topLevelComment']['snippet']['textDisplay']\n","\n","    if comment.find('href') != -1: # Check if comment is referencing a timestamp, do continue if so\n","        continue\n","\n","    comments.append(comment)"]},{"cell_type":"code","execution_count":65,"metadata":{},"outputs":[],"source":["# After collecting comments we can prepare the data in the same way we did when training the model so that the predictions will be applicable\n","\n","def processText(text):\n","    lemmatizer = WordNetLemmatizer()\n","    arr = []\n","\n","    for token in text.split():\n","        if token not in stopwords.words(\"english\") and token.isalnum():\n","            word = lemmatizer.lemmatize(token)\n","            arr.append(word)\n","\n","    return \" \".join(arr)\n","\n","\n","length = len(comments)\n","curr = 0\n","\n","for index, text in enumerate(comments):\n","    comments[index] = processText(text)\n","\n","# After successfully preprocessing the words we can create a Tokenizer fitted on the same data as the trained model and we can finally convert our comments into vector form so they can be evaluated\n","tokenizerData = pd.read_csv('processedTwitterData.csv')\n","tokenizerData.text = tokenizerData.text.astype(str)\n","\n","LENGTH = 280\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(tokenizerData.text)\n","\n","processedComments = pad_sequences(tokenizer.texts_to_sequences(comments), maxlen=280) "]},{"cell_type":"code","execution_count":66,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["3/3 [==============================] - 1s 103ms/step\n","The sentiment of the video is: 0.6633907953898112\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Comments</th>\n","      <th>Predicted Labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Hello</td>\n","      <td>0.952779</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>auhhhhh</td>\n","      <td>0.623955</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>He straight called brofist</td>\n","      <td>0.573565</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Ge mig den</td>\n","      <td>0.210723</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>wahh</td>\n","      <td>0.015373</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                     Comments  Predicted Labels\n","0                       Hello          0.952779\n","1                     auhhhhh          0.623955\n","2  He straight called brofist          0.573565\n","3                  Ge mig den          0.210723\n","4                        wahh          0.015373"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["model = load_model(\"trainedSequentialModel\")\n","prediction = model.predict(np.array(processedComments))\n","\n","results = pd.DataFrame()\n","results['Comments'] = comments\n","results['Predicted Labels'] = prediction\n","\n","sentiment = prediction.sum() / len(prediction)\n","\n","print(\"The sentiment of the video is: \" + str(sentiment))\n","results.head()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPKEUUPx1377maqnIz0FxFt","mount_file_id":"1EDw7oTnmp_OHzesIz0JFBnr5Ag7FzjUo","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.3"}},"nbformat":4,"nbformat_minor":0}
